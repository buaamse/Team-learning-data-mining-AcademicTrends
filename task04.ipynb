{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的package\n",
    "import seaborn as sns #用于画图\n",
    "from bs4 import BeautifulSoup #用于爬取arxiv的数据\n",
    "import re #用于正则表达式，匹配字符串的模式\n",
    "import requests #用于网络连接，发送网络请求，使用域名获取对应信息\n",
    "import json #读取数据，我们的数据为json格式的\n",
    "import pandas as pd #数据处理，数据分析\n",
    "import matplotlib.pyplot as plt #画图工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArxivFile(path, columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n",
    "       'report-no', 'categories', 'license', 'abstract', 'versions',\n",
    "       'update_date', 'authors_parsed'], count=None):\n",
    "    '''\n",
    "    定义读取文件的函数\n",
    "        path: 文件路径\n",
    "        columns: 需要选择的列\n",
    "        count: 读取行数\n",
    "    '''\n",
    "    \n",
    "    data  = []\n",
    "    with open(path, 'r') as f: \n",
    "        for idx, line in enumerate(f): \n",
    "            if idx == count:\n",
    "                break\n",
    "                \n",
    "            d = json.loads(line)\n",
    "            d = {col : d[col] for col in columns}\n",
    "            data.append(d)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "data = readArxivFile('arxiv-metadata-oai-snapshot.json', \n",
    "                     ['id', 'title', 'categories', 'abstract'],\n",
    "                    200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             [hep-ph]\n",
      "1                     [math.CO, cs.CG]\n",
      "2                     [physics.gen-ph]\n",
      "3                            [math.CO]\n",
      "4                   [math.CA, math.FA]\n",
      "                      ...             \n",
      "199995    [astro-ph.CO, gr-qc, hep-th]\n",
      "199996       [hep-ph, hep-ex, nucl-th]\n",
      "199997            [cond-mat.stat-mech]\n",
      "199998              [math.DG, math.AP]\n",
      "199999             [cond-mat.mtrl-sci]\n",
      "Name: categories, Length: 200000, dtype: object\n",
      "0                          [hep-ph]\n",
      "1                        [math, cs]\n",
      "2                         [physics]\n",
      "3                            [math]\n",
      "4                      [math, math]\n",
      "                    ...            \n",
      "199995    [astro-ph, gr-qc, hep-th]\n",
      "199996    [hep-ph, hep-ex, nucl-th]\n",
      "199997                   [cond-mat]\n",
      "199998                 [math, math]\n",
      "199999                   [cond-mat]\n",
      "Name: categories_big, Length: 200000, dtype: object\n",
      "0                          [hep-ph]\n",
      "1                        [math, cs]\n",
      "2                         [physics]\n",
      "3                            [math]\n",
      "4                      [math, math]\n",
      "                    ...            \n",
      "199995    [astro-ph, gr-qc, hep-th]\n",
      "199996    [hep-ph, hep-ex, nucl-th]\n",
      "199997                   [cond-mat]\n",
      "199998                 [math, math]\n",
      "199999                   [cond-mat]\n",
      "Name: categories_big, Length: 200000, dtype: object\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 为了方便数据的处理，我们可以将标题和摘要拼接一起完成分类\n",
    "data['text'] = data['title'] + data['abstract']\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.replace('\\n',' '))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data = data.drop(['abstract', 'title'], axis=1)\n",
    "\n",
    "# 由于原始论文有可能有多个类别，所以也需要处理：\n",
    "# 多个类别，包含子分类\n",
    "data['categories'] = data['categories'].apply(lambda x : x.split(' '))\n",
    "print(data['categories'])\n",
    "\n",
    "# 单个类别，不包含子分类\n",
    "data['categories_big'] = data['categories'].apply(lambda x : [xx.split('.')[0] for xx in x])\n",
    "print(data['categories_big'])\n",
    "print(data['categories_big'].iloc[:])\n",
    "\n",
    "# 然后将类别进行编码，这里类别是多个，所以需要多编码：\n",
    "# 多标签二值化：sklearn.preprocessing.MultiLabelBinarizer(classes=None, sparse_output=False)\n",
    "# classes_属性：若设置classes参数时，其值等于classes参数值，否则从训练集统计标签值\n",
    "# ①classes默认值，classes_属性值从训练集中统计标签值\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "data_label = mlb.fit_transform(data['categories_big'].iloc[:])\n",
    "print(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1338)\t0.07713952249358953\n",
      "  (0, 3234)\t0.08325685818530902\n",
      "  (0, 2529)\t0.05055063593564344\n",
      "  (0, 412)\t0.02966358916483929\n",
      "  (0, 541)\t0.03558328143265654\n",
      "  (0, 3309)\t0.07173629935764876\n",
      "  (0, 3247)\t0.08000722643838502\n",
      "  (0, 1277)\t0.0793668877829128\n",
      "  (0, 3639)\t0.022452149048524696\n",
      "  (0, 3300)\t0.07940495295549563\n",
      "  (0, 2846)\t0.0652037095848\n",
      "  (0, 2924)\t0.0738235826562173\n",
      "  (0, 3669)\t0.060759033487410095\n",
      "  (0, 478)\t0.08277588025033003\n",
      "  (0, 1752)\t0.0817065496375506\n",
      "  (0, 978)\t0.06695254113468696\n",
      "  (0, 686)\t0.08582784041145053\n",
      "  (0, 2076)\t0.04678819616395626\n",
      "  (0, 1273)\t0.04492065018995425\n",
      "  (0, 2849)\t0.14685932651909842\n",
      "  (0, 1152)\t0.13723311109927955\n",
      "  (0, 3301)\t0.05462705182811686\n",
      "  (0, 1159)\t0.0671653764095451\n",
      "  (0, 580)\t0.10664045218016854\n",
      "  (0, 3631)\t0.085727271395169\n",
      "  :\t:\n",
      "  (199999, 2344)\t0.26599002995782295\n",
      "  (199999, 1512)\t0.10837249341120753\n",
      "  (199999, 526)\t0.03456567400311564\n",
      "  (199999, 3971)\t0.07918574429119493\n",
      "  (199999, 2548)\t0.033334257437417185\n",
      "  (199999, 404)\t0.061685279114117406\n",
      "  (199999, 3123)\t0.05179107463679755\n",
      "  (199999, 2607)\t0.054796098622456234\n",
      "  (199999, 3925)\t0.07947115228237013\n",
      "  (199999, 412)\t0.04039735730864302\n",
      "  (199999, 541)\t0.048459089905171865\n",
      "  (199999, 3639)\t0.030576457974116344\n",
      "  (199999, 964)\t0.06355345049620441\n",
      "  (199999, 3964)\t0.060921605840140294\n",
      "  (199999, 3943)\t0.042056347546259296\n",
      "  (199999, 1903)\t0.09017424429177617\n",
      "  (199999, 293)\t0.0344203196283454\n",
      "  (199999, 1569)\t0.04150417377934016\n",
      "  (199999, 2582)\t0.06428496181107439\n",
      "  (199999, 3689)\t0.07546625765148071\n",
      "  (199999, 3640)\t0.1928467057357446\n",
      "  (199999, 1999)\t0.054602048396555185\n",
      "  (199999, 1848)\t0.07068320354585592\n",
      "  (199999, 240)\t0.06927238026114287\n",
      "  (199999, 2538)\t0.17187077080401766\n",
      "x_train:\n",
      "  (0, 3447)\t0.37261021330574906\n",
      "  (0, 1553)\t0.10202175763722755\n",
      "  (0, 39)\t0.10107554482690617\n",
      "  (0, 1184)\t0.11699429012675777\n",
      "  (0, 1045)\t0.08521053753776445\n",
      "  (0, 2803)\t0.0925732237627029\n",
      "  (0, 582)\t0.1152741386874346\n",
      "  (0, 1371)\t0.08550177515774686\n",
      "  (0, 2447)\t0.0902869504722851\n",
      "  (0, 2328)\t0.08680768586490704\n",
      "  (0, 1866)\t0.08443420166090654\n",
      "  (0, 2980)\t0.08080507812920285\n",
      "  (0, 701)\t0.0987908689028818\n",
      "  (0, 1698)\t0.08439543040113681\n",
      "  (0, 2002)\t0.09862254837641932\n",
      "  (0, 1837)\t0.09251789223896231\n",
      "  (0, 3363)\t0.08181155608988563\n",
      "  (0, 3315)\t0.15640443343411062\n",
      "  (0, 3350)\t0.06978054235943418\n",
      "  (0, 2896)\t0.06269462266917895\n",
      "  (0, 2899)\t0.06360106694553982\n",
      "  (0, 2444)\t0.08279033580392532\n",
      "  (0, 3114)\t0.07849878861037364\n",
      "  (0, 1402)\t0.08847572180649486\n",
      "  (0, 403)\t0.09851450765687128\n",
      "  :\t:\n",
      "  (159999, 3829)\t0.0931433919453562\n",
      "  (159999, 152)\t0.19628719155665966\n",
      "  (159999, 1104)\t0.2584247297944289\n",
      "  (159999, 1147)\t0.17515821677638516\n",
      "  (159999, 1198)\t0.2145901966007622\n",
      "  (159999, 812)\t0.10399967868970829\n",
      "  (159999, 1092)\t0.09100620867169443\n",
      "  (159999, 3589)\t0.19258470871195316\n",
      "  (159999, 3297)\t0.07083569480237249\n",
      "  (159999, 2510)\t0.1397676970209403\n",
      "  (159999, 2259)\t0.0998562156201424\n",
      "  (159999, 3586)\t0.08895951631641014\n",
      "  (159999, 2548)\t0.04582464116981924\n",
      "  (159999, 2607)\t0.07532825837186678\n",
      "  (159999, 3925)\t0.10924908237505479\n",
      "  (159999, 412)\t0.05553429250233138\n",
      "  (159999, 3639)\t0.04203349114761335\n",
      "  (159999, 2846)\t0.12207025455339801\n",
      "  (159999, 978)\t0.12534430619593648\n",
      "  (159999, 3689)\t0.03458118307470718\n",
      "  (159999, 3640)\t0.08836886105046073\n",
      "  (159999, 1528)\t0.0390927503225653\n",
      "  (159999, 1848)\t0.03238942645618357\n",
      "  (159999, 240)\t0.0634858793421118\n",
      "  (159999, 2538)\t0.029533867727745456\n",
      "x_test:\n",
      "  (0, 1180)\t0.13933902612722918\n",
      "  (0, 3703)\t0.1515832579830435\n",
      "  (0, 1914)\t0.1371255545840299\n",
      "  (0, 3947)\t0.1197853394462764\n",
      "  (0, 3431)\t0.25730583450725464\n",
      "  (0, 32)\t0.14011346008318673\n",
      "  (0, 2193)\t0.2574477793612475\n",
      "  (0, 2311)\t0.12105120134669643\n",
      "  (0, 64)\t0.1504452000682766\n",
      "  (0, 1582)\t0.14069435622939955\n",
      "  (0, 1290)\t0.11092887001948518\n",
      "  (0, 229)\t0.13959414709493206\n",
      "  (0, 901)\t0.09312823288924887\n",
      "  (0, 3283)\t0.12710724748974941\n",
      "  (0, 3432)\t0.14785115911065408\n",
      "  (0, 3068)\t0.12803806031030673\n",
      "  (0, 2502)\t0.1047357560677597\n",
      "  (0, 226)\t0.12451876851417938\n",
      "  (0, 2135)\t0.11343003722788257\n",
      "  (0, 2371)\t0.19030291703529137\n",
      "  (0, 1869)\t0.12413476305060482\n",
      "  (0, 1360)\t0.11099850793744746\n",
      "  (0, 2116)\t0.13483147682022184\n",
      "  (0, 3125)\t0.1244601523176736\n",
      "  (0, 2720)\t0.11736865586080664\n",
      "  :\t:\n",
      "  (39999, 335)\t0.11441731603915585\n",
      "  (39999, 849)\t0.16621502320649156\n",
      "  (39999, 1321)\t0.11821204321316689\n",
      "  (39999, 3236)\t0.10381171377086391\n",
      "  (39999, 3683)\t0.28595455794160096\n",
      "  (39999, 3829)\t0.07938263951787769\n",
      "  (39999, 2012)\t0.17812250208854508\n",
      "  (39999, 3500)\t0.09356732965250643\n",
      "  (39999, 227)\t0.14750974302561048\n",
      "  (39999, 2362)\t0.13981711994841076\n",
      "  (39999, 2244)\t0.13421279643817294\n",
      "  (39999, 2879)\t0.07672513308417961\n",
      "  (39999, 3796)\t0.125652821145876\n",
      "  (39999, 224)\t0.04486548816964623\n",
      "  (39999, 526)\t0.04049736846621327\n",
      "  (39999, 2528)\t0.09426094128800801\n",
      "  (39999, 3925)\t0.0310363062897177\n",
      "  (39999, 412)\t0.04732980655438477\n",
      "  (39999, 3689)\t0.05894450551536042\n",
      "  (39999, 3640)\t0.17573146473066292\n",
      "  (39999, 1528)\t0.06663458656174302\n",
      "  (39999, 1999)\t0.06397211501578098\n",
      "  (39999, 1848)\t0.027604300325142064\n",
      "  (39999, 240)\t0.054106647493006615\n",
      "  (39999, 2538)\t0.2517061410836012\n",
      "y_train:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "y_test:\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "MultiOutputClassifier(estimator=MultinomialNB())\n"
     ]
    }
   ],
   "source": [
    "# 思路1\n",
    "# 思路1使用TFIDF提取特征，限制最多4000个单词：\n",
    "# tf−idf=tf(t,d)∗idf(t) tf(t,d) 表示在文本 d 中词项 t 出现的词数\n",
    "# idf(t)=ln(1+df(d,t)/1+nd)+1 nd表示训练集文本数，df(d,t)表示包含词项t的文档总数\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=4000)\n",
    "data_tfidf = vectorizer.fit_transform(data['text'].iloc[:])\n",
    "print(data_tfidf)\n",
    "\n",
    "# 由于这里是多标签分类，可以使用sklearn的多标签分类进行封装：\n",
    "# 划分训练集和验证集\n",
    "# 在机器学习中，我们通常将原始数据按照比例分割为“测试集”和“训练集”，从 sklearn.model_selection 中调用train_test_split 函数\n",
    "# 简单用法如下：\n",
    "# X_train,X_test, y_train, y_test =sklearn.model_selection.train_test_split(train_data,train_target,test_size=0.4, random_state=0,stratify=y_train)\n",
    "# train_data：所要划分的样本特征集\n",
    "# train_target：所要划分的样本结果\n",
    "# test_size：样本占比，如果是整数的话就是样本的数量\n",
    "# random_state：是随机数的种子。\n",
    "# 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。\n",
    "# stratify是为了保持split前类的分布。比如有100个数据，80个属于A类，20个属于B类。如果train_test_split(... test_size=0.25, stratify = y_all), 那么split之后数据如下：\n",
    "# training: 75个数据，其中60个属于A类，15个属于B类。\n",
    "# testing: 25个数据，其中20个属于A类，5个属于B类。\n",
    "# 用了stratify参数，training集和testing集的类的比例是 A：B= 4：1，等同于split前的比例（80：20）。通常在这种类分布不平衡的情况下会用到stratify。\n",
    "# 将stratify=X就是按照X中的比例分配\n",
    "# 将stratify=y就是按照y中的比例分配\n",
    "# 整体总结起来各个参数的设置及其类型如下：\n",
    "# 主要参数说明：\n",
    "# *arrays：可以是列表、numpy数组、scipy稀疏矩阵或pandas的数据框\n",
    "# test_size：可以为浮点、整数或None，默认为None\n",
    "# ①若为浮点时，表示测试集占总样本的百分比\n",
    "# ②若为整数时，表示测试样本样本数\n",
    "# ③若为None时，test size自动设置成0.25\n",
    "# train_size：可以为浮点、整数或None，默认为None\n",
    "# ①若为浮点时，表示训练集占总样本的百分比\n",
    "# ②若为整数时，表示训练样本的样本数\n",
    "# ③若为None时，train_size自动被设置成0.75\n",
    "# random_state：可以为整数、RandomState实例或None，默认为None\n",
    "# ①若为None时，每次生成的数据都是随机，可能不一样\n",
    "# ②若为整数时，每次生成的数据都相同\n",
    "# stratify：可以为类似数组或None\n",
    "# ①若为None时，划分出来的测试集或训练集中，其类标签的比例也是随机的\n",
    "# ②若不为None时，划分出来的测试集或训练集中，其类标签的比例同输入的数组中类标签的比例相同，可以用于处理不均衡的数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_tfidf, data_label,\n",
    "                                                 test_size = 0.2, random_state = 1)\n",
    "print(\"x_train:\")                                                   \n",
    "print(x_train)\n",
    "print(\"x_test:\")                                                     \n",
    "print(x_test)\n",
    "print(\"y_train:\")                                                      \n",
    "print(y_train)\n",
    "print(\"y_test:\")                                                    \n",
    "print(y_test)\n",
    "\n",
    "# 构建多标签分类模型\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultiOutputClassifier(MultinomialNB()).fit(x_train, y_train)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.89      7925\n",
      "           1       0.85      0.79      0.82      7339\n",
      "           2       0.77      0.72      0.74      2944\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.72      0.48      0.58      2123\n",
      "           5       0.51      0.66      0.58       987\n",
      "           6       0.86      0.38      0.52       544\n",
      "           7       0.71      0.69      0.70      3649\n",
      "           8       0.76      0.61      0.68      3388\n",
      "           9       0.85      0.88      0.87     10745\n",
      "          10       0.46      0.13      0.20      1757\n",
      "          11       0.79      0.04      0.07       729\n",
      "          12       0.45      0.35      0.39       507\n",
      "          13       0.54      0.36      0.43      1083\n",
      "          14       0.69      0.14      0.24      3441\n",
      "          15       0.84      0.20      0.33       655\n",
      "          16       0.93      0.16      0.27       268\n",
      "          17       0.87      0.43      0.58      2484\n",
      "          18       0.82      0.38      0.52       692\n",
      "\n",
      "   micro avg       0.81      0.65      0.72     51264\n",
      "   macro avg       0.70      0.43      0.50     51264\n",
      "weighted avg       0.80      0.65      0.69     51264\n",
      " samples avg       0.72      0.72      0.70     51264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# sklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息。\n",
    "# 主要参数:\n",
    "# y_true：1维数组，或标签指示器数组/稀疏矩阵，目标值。\n",
    "# y_pred：1维数组，或标签指示器数组/稀疏矩阵，分类器返回的估计值。\n",
    "# labels：array，shape = [n_labels]，报表中包含的标签索引的可选列表。\n",
    "# target_names：字符串列表，与标签匹配的可选显示名称（相同顺序）。\n",
    "# sample_weight：类似于shape = [n_samples]的数组，可选项，样本权重。\n",
    "# digits：int，输出浮点值的位数．\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      "81762    sign reversal of ac josephson current in a fer...\n",
      "44595    mixed-symmetry massless fields in minkowski sp...\n",
      "70766    spin relaxation of localized electrons in n-ty...\n",
      "72830    numerical evidence for unstable magnons at hig...\n",
      "50420    classical and quantized affine models of struc...\n",
      "                               ...                        \n",
      "50057    multicritical points for the spin glass models...\n",
      "98047    spin-orbit-mediated spin relaxation in graphen...\n",
      "5192     measurement of cp-violating asymmetries in b0-...\n",
      "77708    the equilibrium intrinsic crystal-liquid inter...\n",
      "98539    soft processes at the lhc, ii: soft-hard facto...\n",
      "Name: text, Length: 5000, dtype: object\n",
      "x_test:\n",
      "43660    personal recommendation via modified collabora...\n",
      "87278    noise driven translocation of short polymers i...\n",
      "14317    uncertainty principle with quantum fisher info...\n",
      "81932    sieving for mass equidistribution  we approach...\n",
      "95321    critical formation of trapped surfaces in the ...\n",
      "                               ...                        \n",
      "31583    why systems-on-chip needs more uml like a hole...\n",
      "90216    bracket relations for relativity groups  poiss...\n",
      "40603    an approximation ratio for biclustering  the p...\n",
      "3392     exponential decay of correlation for the stoch...\n",
      "87746    first-forbidden continuum- and bound-state bet...\n",
      "Name: text, Length: 95000, dtype: object\n",
      "y_train:\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "y_test:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 思路2\n",
    "# 思路2使用深度学习模型，单词进行词嵌入然后训练。将数据集处理进行编码，并进行截断：\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'].iloc[:100000], data_label[:100000],\n",
    "                                                 test_size = 0.95, random_state = 1)\n",
    "print(\"x_train:\")                                                   \n",
    "print(x_train)\n",
    "print(\"x_test:\")                                                     \n",
    "print(x_test)\n",
    "print(\"y_train:\")                                                      \n",
    "print(y_train)\n",
    "print(\"y_test:\")                                                    \n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x000000B2B9F9DF70>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssequence:\n",
      "[[  0   0   0 ... 118  10   2]\n",
      " [  0   0   0 ...  14   4   4]\n",
      " [  0   0   0 ... 112  56  23]\n",
      " ...\n",
      " [  0   0   0 ...  72  12 374]\n",
      " [ 17 294  10 ...   2   3  57]\n",
      " [  0   0   0 ...  50 391   3]]\n"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "max_features= 500\n",
    "max_len= 150\n",
    "embed_size=100\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "# Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类。\n",
    "# Tokenizer实际上只是生成了一个字典，并且统计了词频等信息，并没有把文本转成需要的向量表示。\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokens = Tokenizer(num_words = max_features)\n",
    "tokens.fit_on_texts(list(data['text'].iloc[:100000]))\n",
    "\n",
    "print(tokens)\n",
    "y_train = data_label[:100000]\n",
    "x_sub_train = tokens.texts_to_sequences(data['text'].iloc[:100000])\n",
    "print(x_sub_train)\n",
    "x_sub_train = sequence.pad_sequences(x_sub_train, maxlen=max_len)\n",
    "\n",
    "print(\"sequence:\")\n",
    "print( x_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 2013s 3s/step - loss: 0.2156 - accuracy: 0.4259 - val_loss: 0.1170 - val_accuracy: 0.6626\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 2081s 3s/step - loss: 0.1155 - accuracy: 0.6651 - val_loss: 0.1052 - val_accuracy: 0.6913\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 2150s 3s/step - loss: 0.1062 - accuracy: 0.6948 - val_loss: 0.1011 - val_accuracy: 0.6951\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 2350s 4s/step - loss: 0.1001 - accuracy: 0.7073 - val_loss: 0.0972 - val_accuracy: 0.7133\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 2186s 3s/step - loss: 0.0963 - accuracy: 0.7149 - val_loss: 0.0951 - val_accuracy: 0.7143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2b2356a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型并完成训练：\n",
    "# LSTM model\n",
    "# Keras Layers:\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D# Keras Callback Functions:\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sequence_input = Input(shape=(max_len, ))\n",
    "x = Embedding(max_features, embed_size, trainable=True)(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "preds = Dense(19, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "model.fit(x_sub_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=0.2,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
